{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-CptVs7iACc"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this lab, we will introduce different ways of learning from sequential data.\n",
    "\n",
    "As a recurring example, we will train neural networks to do **language modelling**, i.e. **predict the next token** in a sentence. In the context of natural language processing a **token could be a character or a word**, but mind you that the concepts introduced here apply to all kinds of sequential data, such as e.g. protein sequences, weather measurements, audio signals, or videos, just to name a few.\n",
    "\n",
    "To really get a grasp of what is going on inside a recurrent neural network (RNN), we will carry out a substantial part of this exercise in Nanograd rather than PyTorch. \n",
    "\n",
    "We start off with a simple toy problem, build an RNN using Nanograd, train it, and see for ourselves that it really works. Once we're convinced, you will implement the Long Short-Term Memory (LSTM) cell, also in Nanograd. \n",
    "\n",
    "This is *not* simple but with the DenseLayer class we already have, it is doable. Having done it yourself will help you understand what happens under the hood of the PyTorch code we will use throughout the course.\n",
    "\n",
    "To summarize, in this notebook we will show you:\n",
    "* How to represent sequences of categorical variables\n",
    "* How to build and train an RNN in Nanograd\n",
    "* How to build and train an LSTM network in Nanograd\n",
    "* How to build and train an LSTM network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XapO8SLwiACd"
   },
   "source": [
    "## Representing tokens or text\n",
    "\n",
    "In previous labs we mainly considered data $x \\in \\mathbb{R}^d$, where $d$ is the feature space dimension.\n",
    "\n",
    "With time sequences our data can be represented as$$x \\in \\mathbb{R}^{t \\, \\times \\, d}$$where $t$ is the sequence length. \n",
    "This emphasises sequence dependence and that the **samples along the sequence are <u>not</u> independent and identically distributed (i.i.d.).**\n",
    "\n",
    "With RNNs, we can model both **many-to-one** functions: \n",
    "$$\\mathbb{R}^{t \\, \\times \\, d} \\rightarrow \\mathbb{R}^c$$\n",
    "and **many-to-many** functions: \n",
    "$$\\mathbb{R}^{t \\, \\times \\, d} \\rightarrow \\mathbb{R}^{t \\, \\times \\, c}$$\n",
    "where $c$ is the amount of classes/output dimensions.\n",
    "\n",
    "There are several ways to represent sequences. With text, the challenge is **how to represent a word** as a feature vector in $d$ dimensions, as we are required to represent text with decimal numbers in order to apply neural networks to it.\n",
    "\n",
    "In **this exercise** we will use a **simple one-hot encoding** but for categorical variables that can take on **many values** (e.g. words in the English language) this may be **infeasible**. \n",
    "\n",
    "For such scenarios, you can project the encodings into a smaller space by use of **embeddings**. If you want to learn more about tokens, encodings and embeddings than what is covered in this exercise, we highly recommend [this lecture](https://www.youtube.com/watch?v=kEMJRjEdNzM&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdA4LPsFiACe"
   },
   "source": [
    "### One-hot encoding over vocabulary\n",
    "\n",
    "One way to represent a fixed amount of words is by making a one-hot encoded vector, which consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify each word.\n",
    "\n",
    "| vocabulary    | one-hot encoded vector   |\n",
    "| ------------- |--------------------------|\n",
    "| Paris         | $= [1, 0, 0, \\ldots, 0]$ |\n",
    "| Rome          | $= [0, 1, 0, \\ldots, 0]$ |\n",
    "| Copenhagen    | $= [0, 0, 1, \\ldots, 0]$ |\n",
    "\n",
    "Representing a large vocabulary with one-hot encodings often becomes **inefficient** because of the **size** of each **sparse vector**.\n",
    "To overcome this challenge it is common practice to \n",
    "- truncate the vocabulary to contain the $k$ most used words\n",
    "- represent the rest with a special symbol, $\\mathtt{UNK}$, to define **unknown/unimportant** words.\n",
    "\n",
    "This often causes entities such as *names* to be represented with $\\mathtt{UNK}$ because they are *rare*.\n",
    "\n",
    "Consider the following text\n",
    "> I love the corny jokes in Spielberg's new movie.\n",
    "\n",
    "where an example result would be similar to\n",
    "> I love the corny jokes in $\\mathtt{UNK}$'s new movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNmyPw7zk2gY"
   },
   "source": [
    "## Generating a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9IEA4t2k2gb"
   },
   "source": [
    "For this exercise we will create a simple dataset that we can learn from. We generate sequences of the form:\n",
    "\n",
    "`a b EOS`,\n",
    "\n",
    "`a a b b EOS`,\n",
    "\n",
    "`a a a a a b b b b b EOS`\n",
    "\n",
    "where `EOS` is a special character denoting the end of a sequence. The task is to predict the next token $t_n$, i.e. `a`, `b`, `EOS` or the unknown token `UNK` given a sequence of tokens $\\{ t_{1}, t_{2}, \\dots , t_{n-1}\\}$, and we are to process sequences in a sequential manner. As such, the network will need to learn that e.g. $x$ `b`s and an `EOS` token will follow $x$ `a`s ($x \\in \\{1, 2, 3, 4\\}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "id": "dcoN-kb7k2gc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set seed such that we always get the same dataset\n",
    "# (this is a good idea in general)\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=2**5):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 4)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMLd3Gzak2gp"
   },
   "source": [
    "## Representing tokens as indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9LSqaJSk2gp"
   },
   "source": [
    "To build a one-hot encoding, we need to **assign each possible word in our vocabulary an index**. \n",
    "\n",
    "We do that by creating two dictionaries: \n",
    "1. one that allows us to go from a given word to its corresponding index in our vocabulary (`word_to_idx`)\n",
    "2. one for the reverse direction (`idx_to_word`).\n",
    "\n",
    "The keyword `vocab_size` specifies the maximum size of our vocabulary. I\n",
    "\n",
    "f we try to access a word that does not exist in our vocabulary, it is automatically replaced by the `UNK` token or its corresponding index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNY1OOS_k2gy"
   },
   "source": [
    "## Exercise a) Sequence to dictionary function \n",
    "\n",
    "Complete the sequences_to_dicts function below. You will need to fill the `word_to_idx` and `idx_to_word` dictionaries so that we can go back and forth between the two representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "Smdo70UMk2gr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 32 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The word corresponding to index 1 is 'b'\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    \"\"\"\n",
    "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
    "    \"\"\"\n",
    "    # A bit of Python-magic to flatten a nested list\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    #\n",
    "    # ->    defaultdict is like a normal dictionary, but if we try to access a key that does not exist,\n",
    "    #       it will create it and assign it the value given in the constructor.\n",
    "    #\n",
    "    # ->    the default value of int() is 0, so if we try to access a non-existing key, \n",
    "    #       it will be created and assigned the value 0.\n",
    "    #\n",
    "    # Example:\n",
    "    #       d = defaultdict(int)\n",
    "    #       print(d['non_existing_key'])  # prints 0\n",
    "\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "        # If the word doesn't exist in the dictionary, it is created and assigned the value 0\n",
    "        # (because of defaultdict(int)), and then we add 1 to it.\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(\n",
    "        list(word_count.items()), \n",
    "        key=lambda l: -l[1]\n",
    "    )\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    # (sorted returns a list of tuples (word, count)\n",
    "    #   so we take only the first element of each tuple)\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: vocab_size-1)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "# 2^5 sequences\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)\n",
    "\n",
    "print(f'We have {num_sequences} sentences and {len(word_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', word_to_idx['b'])\n",
    "print(f'The word corresponding to index 1 is \\'{idx_to_word[1]}\\'')\n",
    "\n",
    "assert idx_to_word[word_to_idx['b']] == 'b', \\\n",
    "    'Consistency error: something went wrong in the conversion.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGSoDRgHk2g1"
   },
   "source": [
    "## Partitioning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMTn1iLIk2g1"
   },
   "source": [
    "To build our dataset, we need to \n",
    "- create inputs and targets for each sequences\n",
    "- partition sentences it into training, validation and test sets. 80%, 10% and 10% is a common distribution, but mind you that this largely depends on the size of the dataset. \n",
    "\n",
    "Since we are doing next-word predictions, our target sequence is simply the input sequence shifted by one word.\n",
    "\n",
    "We can use PyTorch's `Dataset` class to build a simple dataset where we can easily retrieve (inputs, targets) pairs for each of our sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "9dW7MrPnk2g3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 25 samples in the training set.\n",
      "We have 3 samples in the validation set.\n",
      "We have 3 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets \n",
    "        #   s.t. both lists contain L-1 words of a sentence of length L\n",
    "        #   but \n",
    "        #   targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    # (lists of lists)\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    #   -> dataset_class is a placeholder for the Dataset class defined above\n",
    "    #   -> so we basically call the constructor of the Dataset class\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)\n",
    "\n",
    "print(f'We have {len(training_set)} samples in the training set.')\n",
    "print(f'We have {len(validation_set)} samples in the validation set.')\n",
    "print(f'We have {len(test_set)} samples in the test set.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xMMSm7Mk2g9"
   },
   "source": [
    "When working with more complex data than what we use in this exercise, creating a **PyTorch `DataLoader`** on top of the dataset can be **beneficial**. \n",
    "\n",
    "A data loader is basically a fancy generator/iterator that we can use to abstract away all of the data handling and pre-processing + it's super useful for processing batches of data as well! Data loaders will come in handy later when you start to work on your projects, so be sure to check them out!\n",
    "\n",
    "For more information on how to use datasets and data loaders in PyTorch, [consult the official guide](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-rfgDfZeMQ6"
   },
   "source": [
    "## Nanograd utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRO5ssg0eQMK"
   },
   "source": [
    "We load necessary utility functions for the Nanograd library, which we saw in Week 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "Jd4CoEBNzNWS"
   },
   "outputs": [],
   "source": [
    "# Copy and pasted from https://github.com/rasmusbergpalm/nanograd/blob/main/nanograd.py\n",
    "\n",
    "from math import exp, log, tanh\n",
    "\n",
    "class Var:\n",
    "    \"\"\"\n",
    "    A variable which holds a float and enables gradient computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, val: float, grad_fn=lambda: []):\n",
    "        assert type(val) == float\n",
    "        self.v = val\n",
    "        self.grad_fn = grad_fn\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def backprop(self, bp):\n",
    "        # bp is the backpropagated gradient\n",
    "        self.grad += bp\n",
    "        for input, grad in self.grad_fn():\n",
    "            input.backprop(grad * bp)\n",
    "\n",
    "    def backward(self):\n",
    "        self.backprop(1.0)\n",
    "\n",
    "    def __add__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(\n",
    "            self.v + other.v,                   # value\n",
    "            lambda: [(self, 1.0), (other, 1.0)] # d(x+y)/dx = 1, d(x+y)/dy = 1\n",
    "        )\n",
    "\n",
    "    def __mul__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return Var(self.v * other.v, lambda: [(self, other.v), (other, self.v)])\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        assert type(power) in {float, int}, \"power must be float or int\"\n",
    "        return Var(self.v ** power, lambda: [(self, power * self.v ** (power - 1))])\n",
    "\n",
    "    def __neg__(self: 'Var') -> 'Var':\n",
    "        return Var(-1.0) * self\n",
    "\n",
    "    def __sub__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self: 'Var', other: 'Var') -> 'Var':\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Var(v=%.4f, grad=%.4f)\" % (self.v, self.grad)\n",
    "    \n",
    "    def exp(self):\n",
    "        return Var(exp(self.v), lambda: [(self, exp(self.v))])\n",
    "    \n",
    "    def log(self):\n",
    "        return Var(log(self.v), lambda: [(self, self.v ** -1)])\n",
    "\n",
    "    def relu(self):\n",
    "        return Var(self.v if self.v > 0.0 else 0.0, lambda: [(self, 1.0 if self.v > 0.0 else 0.0)])\n",
    "    \n",
    "    def identity(self):\n",
    "        return self\n",
    "\n",
    "    def sigmoid(self):\n",
    "        return Var(0.5) * (Var(1.0) + (Var(0.5) * self).tanh()) # logistic function is a scaled and shifted version of tanh\n",
    "    \n",
    "    def tanh(self):\n",
    "        return Var(tanh(self.v), lambda: [(self, 1-tanh(self.v) ** 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "9AMqMsiseMfz"
   },
   "outputs": [],
   "source": [
    "# convert from ndarray to Var\n",
    "#     1. vector to matrix\n",
    "#     2. matrix to matrix\n",
    "def nparray_to_Var(x):\n",
    "  # 1. -> vector to matrix\n",
    "  if x.ndim==1:\n",
    "    y = [[Var(float(x[i]))] for i in range(x.shape[0])] # always work with list of list\n",
    "  # 2. -> matrix to matrix\n",
    "  else:\n",
    "    y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n",
    "  return y\n",
    "\n",
    "# convert from Var to ndarray  \n",
    "def Var_to_nparray(x):\n",
    "  try:\n",
    "    # Try creating a matrix of size (n, m) \n",
    "    # where\n",
    "    #   n=number of observations,\n",
    "    #   m=number of features\n",
    "    y = np.zeros((len(x),len(x[0])))\n",
    "    for i in range(len(x)):\n",
    "      for j in range(len(x[0])):\n",
    "        y[i,j] = x[i][j].v\n",
    "  # If that fails, try creating a vector of size (n,)\n",
    "  # where n=number of observations\n",
    "  except TypeError:\n",
    "    y = np.zeros((len(x)))\n",
    "    for i in range(len(x)):\n",
    "      y[i] = x[i].v\n",
    "\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "ij_ieRsAt7Xt"
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    raise NotImplementedError\n",
    "    # We will implement this in the Normal and Constant initializers\n",
    "\n",
    "  def init_bias(self, n_out):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "eb18N5phuIha"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class NormalInitializer(Initializer):\n",
    "\n",
    "  def __init__(self, mean=0, std=0.1):\n",
    "    self.mean = mean\n",
    "    self.std = std\n",
    "\n",
    "  # Initialise weights with samples from a normal distribution\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    return [[Var(random.gauss(self.mean, self.std)) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "  # Initialize biases to zero\n",
    "  def init_bias(self, n_out):\n",
    "    return [Var(0.0) for _ in range(n_out)]\n",
    "\n",
    "class ConstantInitializer(Initializer):\n",
    "\n",
    "  def __init__(self, weight=1.0, bias=0.0):\n",
    "    self.weight = weight\n",
    "    self.bias = bias\n",
    "\n",
    "  # Initialize weights to a constant value\n",
    "  def init_weights(self, n_in, n_out):\n",
    "    return [[Var(self.weight) for _ in range(n_out)] for _ in range(n_in)]\n",
    "\n",
    "  # Initialize biases to a constant value\n",
    "  def init_bias(self, n_out):\n",
    "    return [Var(self.bias) for _ in range(n_out)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzmryk72k2g-"
   },
   "source": [
    "## One-hot encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abRN9f8Xk2g_"
   },
   "source": [
    "We now create a simple function that returns the one-hot encoded representation of a given index of a word in our vocabulary. \n",
    "\n",
    "Notice that **the shape of the one-hot encoding is equal to the entire vocabulary** (which can be huge!). \n",
    "\n",
    "Additionally, we define a function to automatically one-hot encode a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "IZruCIHJk2hB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our one-hot encoding of 'a' has shape (1, 4).\n",
      "Our one-hot encoding of 'a b' has shape (2, 4).\n",
      "[[Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]]\n",
      "[[Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)], [Var(v=0.0000, grad=0.0000), Var(v=1.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/qw26xkk51853rtytqd9ns_5r0000gn/T/ipykernel_48735/161328427.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.array([np.zeros(vocab_size)])\n",
    "    # ! shape of the one-hot encoding is equal to the entire vocabulary\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[0][idx] = 1.0\n",
    "    return nparray_to_Var(one_hot)\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([Var_to_nparray(one_hot_encode(word_to_idx[word], vocab_size)) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[2], 1)\n",
    "    return nparray_to_Var(encoding)\n",
    "\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {Var_to_nparray(test_word).shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {Var_to_nparray(test_sentence).shape}.')\n",
    "\n",
    "print(test_word)\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "JT6BqYrU_NxQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/qw26xkk51853rtytqd9ns_5r0000gn/T/ipykernel_48735/161328427.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n"
     ]
    }
   ],
   "source": [
    "encoded_training_set_x = []\n",
    "encoded_training_set_y = []\n",
    "encoded_validation_set_x = []\n",
    "encoded_validation_set_y = []\n",
    "encoded_test_set_x = []\n",
    "encoded_test_set_y = []\n",
    "\n",
    "for n in range(len(training_set)):\n",
    "  encoded_training_set_x.append(one_hot_encode_sequence(training_set[n][0], vocab_size))\n",
    "  encoded_training_set_y.append(one_hot_encode_sequence(training_set[n][1], vocab_size))\n",
    "for n in range(len(validation_set)):\n",
    "  encoded_validation_set_x.append(one_hot_encode_sequence(validation_set[n][0], vocab_size))\n",
    "  encoded_validation_set_y.append(one_hot_encode_sequence(validation_set[n][1], vocab_size))\n",
    "for n in range(len(test_set)):\n",
    "  encoded_test_set_x.append(one_hot_encode_sequence(test_set[n][0], vocab_size))\n",
    "  encoded_test_set_y.append(one_hot_encode_sequence(test_set[n][1], vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "erI_MXvKk2hG"
   },
   "source": [
    "Great! Now that we have our one-hot encodings in place, we can move on to the RNNs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA6bxjGWjeSB"
   },
   "source": [
    "# Introduction to Recurrent Neural Networks (RNN)\n",
    "\n",
    "Reading material: [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and (optionally) [this lecture](https://www.youtube.com/watch?v=iWea12EAu6U&list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z).\n",
    "\n",
    "___\n",
    "\n",
    "A recurrent neural network (RNN) is a type of neural network that has been succesful in modelling sequential data, e.g. language, speech, protein sequences, etc.\n",
    "\n",
    "A RNN performs its computations in a cyclic manner, where the same computation is applied to every sample of a given sequence.\n",
    "The idea is that the network should be able to use the previous computations as some form of memory and apply this to future computations.\n",
    "An image may best explain how this is to be understood,\n",
    "\n",
    "![rnn-unroll image](https://github.com/DeepLearningDTU/02456-2025/blob/master/static_files/rnn-unfold.png?raw=1)\n",
    "\n",
    "\n",
    "where it the network contains the following elements:\n",
    "\n",
    "- $x$ is the input sequence of samples, \n",
    "- $U$ is a weight matrix applied to the given input sample,\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output),\n",
    "- $h$ is the hidden state (the network's memory) for a given time step, and\n",
    "- $o$ is the resulting output.\n",
    "\n",
    "When the network is unrolled as shown, it is easier to refer to a timestep, $t$.\n",
    "We have the following computations through the network:\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ is a non-linear activation function, e.g. $\\mathrm{tanh}$.\n",
    "- $o_t = W\\,{h_t}$\n",
    "\n",
    "---\n",
    "\n",
    "When we are doing **language modelling using a cross-entropy loss**, we additionally apply the **softmax** function to the output $o_{t}$:\n",
    "\n",
    "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
    "\n",
    "---\n",
    "\n",
    "### Backpropagation through time\n",
    "\n",
    "We define a loss function\n",
    "\n",
    "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\n",
    "\n",
    "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function.\n",
    "\n",
    "Backpropagation through time amounts to computing the gradients of the loss using the same type of clever bookkeeping we applied to the feed-forward network in week 1. This you will do in Exercise D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuvwbvsGz9KE"
   },
   "source": [
    "## Implementing an RNN\n",
    "\n",
    "We will implement the forward pass, backward pass, optimization and training loop for an RNN in Nanograd so that you can get familiar with the recurrent nature of RNNs. Later, we will go back to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfbfcB-NJZuM"
   },
   "source": [
    "We define the Nanograd DenseLayer class from [lab 2](https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/2_Feedforward_Python/2.1-EXE-FNN-AutoDif-Nanograd.ipynb) with a few additions:\n",
    "* the option use_bias to define a layer without bias. This is useful when we define the recurrent layer and\n",
    "* a method forward_sequence which is useful when a DenseLayer is used as part of a recurrent neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "TqkVyEEACHKS"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, n_in: int, n_out: int, act_fn, initializer = NormalInitializer(), use_bias=True):\n",
    "        self.weights = initializer.init_weights(n_in, n_out)\n",
    "        self.use_bias = use_bias\n",
    "        if use_bias:\n",
    "          self.bias = initializer.init_bias(n_out)\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):    \n",
    "        return 'Weights: ' + repr(self.weights) + (' Biases: ' + repr(self.bias) if self.use_bias else '')\n",
    "\n",
    "    def parameters(self) -> Sequence[Var]:\n",
    "      params = []\n",
    "      for r in self.weights:\n",
    "        params += r\n",
    "\n",
    "      if self.use_bias:\n",
    "        params += self.bias\n",
    "\n",
    "      return params\n",
    "\n",
    "    def forward(self, input: Sequence[Var]) -> Sequence[Var]:\n",
    "        \n",
    "        # self.weights is a matrix with dimension n_in x n_out. We check that the dimensionality of the input \n",
    "        # to the current layer matches the number of nodes in the current layer\n",
    "        assert len(self.weights) == len(input), \"weights and input must match in first dimension\"\n",
    "        \n",
    "        weights = self.weights\n",
    "        out = []\n",
    "\n",
    "        # For some given data point single_input, we now want to calculate the resulting value in each node in the current layer\n",
    "        \n",
    "        # Loop over the (number of) nodes in the current layer:\n",
    "        for j in range(len(weights[0])): \n",
    "            \n",
    "            # Initialize the node value depending on its corresponding parameters.\n",
    "            node = self.bias[j] if self.use_bias else Var(0.0)\n",
    "\n",
    "            # We now finish the linear transformation corresponding to the parameters of the currently considered node.\n",
    "            for i in range(len(input)):\n",
    "                node += input[i]*weights[i][j]\n",
    "\n",
    "            # Finally, we apply the non-linear activation function\n",
    "            node = self.act_fn(node)\n",
    "\n",
    "            # Append the computed node value to the output list\n",
    "            out.append(node)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]]) -> Sequence[Sequence[Var]]:\n",
    "        out = []\n",
    "        for i in range(len(input)): \n",
    "            node = self.forward(input[i])\n",
    "            out.append(node)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDKFjjQEM-xX"
   },
   "source": [
    "## Exercise b) The RNNLayer class\n",
    "\n",
    "Complete the RNNLayer class below.\n",
    "\n",
    "- Explain how we reuse the `DenseLayer` class.\n",
    "- Explain what the `forward` and the `forward_sequence` method do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "id": "IcM1N6PQrT7l"
   },
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class RNNLayer:\n",
    "    def __init__(self, n_in: int, n_hid: int, act_fn, initializer = NormalInitializer(), initializer_hid = NormalInitializer()):\n",
    "        self.n_hid = n_hid\n",
    "        self.in_hid_layer = DenseLayer(n_in, n_hid, lambda x: x, initializer)                           # use a bias: U*x_t + b\n",
    "        self.hid_hid_layer = DenseLayer(n_hid, n_hid, lambda x: x, initializer_hid, use_bias=False)     # V*h_{t-1} -> no bias! \n",
    "        # -> These two together form the affine transformation U*x_t + V*h_{t-1} + b\n",
    "        # -> After this, we apply the non-linear activation function\n",
    "        self.initial_hid = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.stored_hid = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):    \n",
    "        return 'Feed-forward: ' + repr(self.in_hid_layer) + ' Recurrent: ' + repr(self.hid_hid_layer) + ' Initial hidden: ' + repr(self.initial_hid)\n",
    "\n",
    "    def parameters(self) -> Sequence[Var]:      \n",
    "      return self.in_hid_layer.parameters() + self.hid_hid_layer.parameters() + self.initial_hid\n",
    "\n",
    "    def forward_step(self, input: Sequence[Var], input_hid: Sequence[Var]) -> Sequence[Var]:\n",
    "        in_hids = self.in_hid_layer.forward(input)          # contribution from input (+ bias)\n",
    "        hid_hids = self.hid_hid_layer.forward(input_hid)    # contribution from hidden state (no bias)\n",
    "\n",
    "        hids = []\n",
    "        for i in range(self.n_hid):\n",
    "          hids.append(self.act_fn(in_hids[i] + hid_hids[i]))\n",
    "\n",
    "        return hids\n",
    "    \n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]], use_stored_hid = False) -> Sequence[Sequence[Var]]:\n",
    "        out = []\n",
    "        if use_stored_hid:\n",
    "            hid = self.stored_hid\n",
    "        else:\n",
    "            hid = self.initial_hid\n",
    "        # Takes a sequence and loops over each character in the sequence. Note that each character has dimension equal to the embedding dimension\n",
    "        for i in range(len(input)):\n",
    "            # Compute h_t based on x_t and h_{t-1}\n",
    "            hid = self.forward_step(input[i], hid)\n",
    "            # here, the hid argument is\n",
    "            #   -> either the initial hidden state (if use_stored_hid is False)\n",
    "            #   -> or the stored hidden state h_{t-1} (if use_stored_hid is True)\n",
    "            out.append(hid)\n",
    "        self.stored_hid = hid\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgAU6qPHKJFr"
   },
   "source": [
    "Now we can define a network and pass some data through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "MFkZ5gNG6d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed-forward: Weights: [[Var(v=-0.0348, grad=0.0000), Var(v=0.0992, grad=0.0000), Var(v=0.0037, grad=0.0000), Var(v=0.1276, grad=0.0000), Var(v=-0.0439, grad=0.0000)]] Biases: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)] Recurrent: Weights: [[Var(v=0.0797, grad=0.0000), Var(v=0.0525, grad=0.0000), Var(v=-0.2447, grad=0.0000), Var(v=-0.0406, grad=0.0000), Var(v=-0.0237, grad=0.0000)], [Var(v=-0.0627, grad=0.0000), Var(v=-0.0893, grad=0.0000), Var(v=0.1590, grad=0.0000), Var(v=-0.0120, grad=0.0000), Var(v=0.0790, grad=0.0000)], [Var(v=-0.1339, grad=0.0000), Var(v=-0.2078, grad=0.0000), Var(v=-0.0473, grad=0.0000), Var(v=0.0407, grad=0.0000), Var(v=-0.0723, grad=0.0000)], [Var(v=0.0530, grad=0.0000), Var(v=0.0802, grad=0.0000), Var(v=-0.0449, grad=0.0000), Var(v=-0.0060, grad=0.0000), Var(v=-0.0738, grad=0.0000)], [Var(v=0.1080, grad=0.0000), Var(v=0.1768, grad=0.0000), Var(v=0.0497, grad=0.0000), Var(v=-0.0505, grad=0.0000), Var(v=-0.0705, grad=0.0000)]] Initial hidden: [Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000), Var(v=0.0000, grad=0.0000)]\n",
      "[[[Var(v=0.0336, grad=1.0000)], [Var(v=0.0647, grad=0.0000)], [Var(v=0.0945, grad=0.0000)]], [[Var(v=0.0336, grad=0.0000)], [Var(v=0.0647, grad=0.0000)], [Var(v=0.0945, grad=0.0000)]]]\n",
      "These are the outputs for each batch, sequence, and feature:\n",
      "Batch 0:\n",
      "  Sequence 0:\n",
      "    Feature 0: 0.03361958397252805 (grad: 1.0)\n",
      "  Sequence 1:\n",
      "    Feature 0: 0.06466060942579821 (grad: 0.0)\n",
      "  Sequence 2:\n",
      "    Feature 0: 0.09446135964836855 (grad: 0.0)\n",
      "Batch 1:\n",
      "  Sequence 0:\n",
      "    Feature 0: 0.03361958397252805 (grad: 0.0)\n",
      "  Sequence 1:\n",
      "    Feature 0: 0.06466060942579821 (grad: 0.0)\n",
      "  Sequence 2:\n",
      "    Feature 0: 0.09446135964836855 (grad: 0.0)\n"
     ]
    }
   ],
   "source": [
    "NN = [\n",
    "    RNNLayer(1, 5, lambda x: x.tanh()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "def forward_batch(input: Sequence[Sequence[Sequence[Var]]], network, use_stored_hid=False):\n",
    "\n",
    "  # use_stored_hid is always False during training to prevent infinite cycles\n",
    "  #   and to make sure that each minibatch treats each sequence independently\n",
    "  #\n",
    "  #     but\n",
    "  #\n",
    "  # you could use it during inference/generation to maintain context accross batches\n",
    "\n",
    "  def forward_single_sequence(x, network, use_stored_hid):\n",
    "    for layer in network:\n",
    "        if isinstance(layer, RNNLayer):\n",
    "            x = layer.forward_sequence(x, use_stored_hid) \n",
    "        else:\n",
    "            x = layer.forward_sequence(x)\n",
    "    return x\n",
    "\n",
    "  output = [ forward_single_sequence(input[n], network, use_stored_hid) for n in range(len(input))]\n",
    "  return output\n",
    "\n",
    "print(NN[0])\n",
    "x_train =[\n",
    "          [\n",
    "              [Var(1.0)], [Var(2.0)], [Var(3.0)] # sequence 1 in batch 1\n",
    "          ], # batch 1\n",
    "          [\n",
    "              [Var(1.0)], [Var(2.0)], [Var(3.0)] # sequence 1 in batch 2\n",
    "          ]  # batch 2\n",
    "          ]\n",
    "\n",
    "output_train = forward_batch(x_train, NN)    \n",
    "# -> first dimension  = batch\n",
    "# -> second dimension = sequence\n",
    "# -> third dimension  = feature\n",
    "output_train[0][0][0].backward()\n",
    "\n",
    "print(output_train)\n",
    "\n",
    "# Print the output in an interpretable format with comments\n",
    "print(\"These are the outputs for each batch, sequence, and feature:\")\n",
    "for i, batch in enumerate(output_train):\n",
    "    print(f\"Batch {i}:\")\n",
    "    for j, seq in enumerate(batch):\n",
    "        print(f\"  Sequence {j}:\")\n",
    "        for k, var in enumerate(seq):\n",
    "            print(f\"    Feature {k}: {var.v} (grad: {var.grad})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yolo5dKrk2hR"
   },
   "source": [
    "## Exercise c) Unit test\n",
    "\n",
    "Make unit tests to make sure that the output and the backward method work as it should.\n",
    "\n",
    "NOTE: The .backward() call above simply backpropagates a value in the output (and not a loss). Below, we will extend our loss functions to be able to handle backpropagation through time.\n",
    "\n",
    "Recycling code from [Week 2](https://github.com/DeepLearningDTU/02456-2025/tree/master/Week2%20Feedforward%20Python) is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "id": "GhCB1ASwK3X7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RNN forward pass test passed\n",
      "✓ RNN backward pass test passed - 10 parameters have gradients\n",
      "✓ Sequence loss tests passed\n",
      "\n",
      "✓ All unit tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Unit tests for RNN implementation\n",
    "\n",
    "def test_rnn_forward():\n",
    "    \"\"\"Test that RNN forward pass produces correct shapes and basic functionality\"\"\"\n",
    "    \n",
    "    # Create a simple RNN\n",
    "    rnn = RNNLayer(2, 3, lambda x: x.tanh())  # 2 input features, 3 hidden units\n",
    "    \n",
    "    # Test single step forward\n",
    "    input_step = [Var(1.0), Var(0.5)]\n",
    "    hidden_step = [Var(0.0), Var(0.0), Var(0.0)]\n",
    "    \n",
    "    output_step = rnn.forward_step(input_step, hidden_step)\n",
    "    \n",
    "    # Check output shape\n",
    "    assert len(output_step) == 3, f\"Expected 3 hidden units, got {len(output_step)}\"\n",
    "    \n",
    "    # Test sequence forward\n",
    "    input_sequence = [\n",
    "        [Var(1.0), Var(0.5)],\n",
    "        [Var(0.5), Var(1.0)],\n",
    "        [Var(0.0), Var(0.5)]\n",
    "    ]\n",
    "    \n",
    "    output_sequence = rnn.forward_sequence(input_sequence)\n",
    "    \n",
    "    # Check sequence output shape\n",
    "    assert len(output_sequence) == 3, f\"Expected sequence length 3, got {len(output_sequence)}\"\n",
    "    assert len(output_sequence[0]) == 3, f\"Expected 3 hidden units per step, got {len(output_sequence[0])}\"\n",
    "    \n",
    "    print(\"✓ RNN forward pass test passed\")\n",
    "\n",
    "def test_rnn_backward():\n",
    "    \"\"\"Test that backward pass works and gradients are computed\"\"\"\n",
    "    \n",
    "    # Create simple RNN and data\n",
    "    rnn = RNNLayer(1, 2, lambda x: x.tanh())\n",
    "    input_seq = [[Var(1.0)], [Var(0.5)]]\n",
    "    \n",
    "    # Forward pass\n",
    "    output = rnn.forward_sequence(input_seq)\n",
    "    \n",
    "    # Compute a simple loss (sum of all outputs)\n",
    "    loss = Var(0.0)\n",
    "    for step in output:\n",
    "        for unit in step:\n",
    "            loss += unit\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check that gradients exist\n",
    "    params = rnn.parameters()\n",
    "    gradient_count = 0\n",
    "    for param in params:\n",
    "        if param.grad is not None:\n",
    "            gradient_count += 1\n",
    "    \n",
    "    assert gradient_count > 0, \"No gradients computed during backward pass\"\n",
    "    print(f\"✓ RNN backward pass test passed - {gradient_count} parameters have gradients\")\n",
    "\n",
    "def test_sequence_loss():\n",
    "    \"\"\"Test sequence loss functions\"\"\"\n",
    "    \n",
    "    # Create simple target and prediction sequences (list of (target, prediction) pairs)\n",
    "    target = [\n",
    "        [[Var(1.0), Var(0.0)], [Var(0.0), Var(1.0)]],  # sequence 1\n",
    "        [[Var(0.0), Var(1.0)], [Var(1.0), Var(0.0)]]   # sequence 2\n",
    "    ]\n",
    "    \n",
    "    prediction = [\n",
    "        [[Var(0.8), Var(0.2)], [Var(0.1), Var(0.9)]],  # sequence 1\n",
    "        [[Var(0.3), Var(0.7)], [Var(0.6), Var(0.4)]]   # sequence 2\n",
    "    ]\n",
    "    \n",
    "    # Test squared loss\n",
    "    sq_loss = squared_loss_sequence(target, prediction)\n",
    "    assert sq_loss.v > 0, \"Squared loss should be positive\"\n",
    "    \n",
    "    # Test cross entropy loss\n",
    "    ce_loss = cross_entropy_loss_sequence(target, prediction)\n",
    "    assert ce_loss.v > 0, \"Cross entropy loss should be positive\"\n",
    "    \n",
    "    print(\"✓ Sequence loss tests passed\")\n",
    "\n",
    "# Run the tests\n",
    "test_rnn_forward()\n",
    "test_rnn_backward()\n",
    "test_sequence_loss()\n",
    "\n",
    "print(\"\\n✓ All unit tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4d4_2b6mK5jH"
   },
   "source": [
    "## Exercise d) Advanced initialization\n",
    "\n",
    "How can we use He initialization for the recurrent layer?\n",
    "\n",
    "Hint: the sum of two unit variance stochastic variables have variance 2.\n",
    "\n",
    "Insert code for He initialization of the recurrent layer. Again, recycling code from Lab 2 is fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "id": "oRn3mDnzLxu2"
   },
   "outputs": [],
   "source": [
    "## He\n",
    "def DenseLayer_He_tanh(n_in: int, n_out: int):\n",
    "    # For tanh activation, He initialization uses sqrt(2/n_in)\n",
    "    # But for RNN, we need to account for the sum of input and recurrent contributions\n",
    "    # Since we have both input and hidden contributions, and they have unit variance,\n",
    "    # their sum has variance 2, so we use sqrt(1/n_in) to compensate\n",
    "    std = (1.0 / n_in) ** 0.5  # He initialization for tanh with recurrent considerations\n",
    "    return DenseLayer(n_in, n_out, lambda x: x.tanh(), initializer = NormalInitializer(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozNN9xXML0yc"
   },
   "source": [
    "## Exercise e) Sequence loss function\n",
    "\n",
    "We want to solve a **sequence to sequence** (encoder-decoder) problem. So you need a **sequence loss function.**\n",
    "\n",
    "Implement the function such that the sequence loss can \n",
    "- take flexible input dimensions \n",
    "- take any loss as an argument, such as squared loss and cross entropy. (We recommend using cross entropy below)\n",
    "\n",
    "We have provided a bit of code to try it out.\n",
    "\n",
    "Hints: You can get inspiration from the forward_sequence method above. You can copy and paste squared loss and cross entropy from Lab 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "bYpEnbeMP4yL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[Var(v=-0.0042, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0074, grad=0.0000), Var(v=0.0019, grad=0.0000)], [Var(v=-0.0035, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0059, grad=0.0000), Var(v=0.0021, grad=0.0000)], [Var(v=-0.0036, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0061, grad=0.0000), Var(v=0.0021, grad=0.0000)], [Var(v=0.0056, grad=0.0000), Var(v=-0.0054, grad=0.0000), Var(v=-0.0158, grad=0.0000), Var(v=0.0127, grad=0.0000)], [Var(v=0.0057, grad=0.0000), Var(v=-0.0055, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.0128, grad=0.0000)], [Var(v=0.0057, grad=0.0000), Var(v=-0.0055, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.0128, grad=0.0000)]], [[Var(v=-0.0042, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0074, grad=0.0000), Var(v=0.0019, grad=0.0000)], [Var(v=0.0056, grad=0.0000), Var(v=-0.0054, grad=0.0000), Var(v=-0.0159, grad=0.0000), Var(v=0.0127, grad=0.0000)]], [[Var(v=-0.0042, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0074, grad=0.0000), Var(v=0.0019, grad=0.0000)], [Var(v=-0.0035, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0059, grad=0.0000), Var(v=0.0021, grad=0.0000)], [Var(v=-0.0036, grad=0.0000), Var(v=-0.0013, grad=0.0000), Var(v=0.0061, grad=0.0000), Var(v=0.0021, grad=0.0000)], [Var(v=0.0056, grad=0.0000), Var(v=-0.0054, grad=0.0000), Var(v=-0.0158, grad=0.0000), Var(v=0.0127, grad=0.0000)], [Var(v=0.0057, grad=0.0000), Var(v=-0.0055, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.0128, grad=0.0000)], [Var(v=0.0057, grad=0.0000), Var(v=-0.0055, grad=0.0000), Var(v=-0.0161, grad=0.0000), Var(v=0.0128, grad=0.0000)]]]\n",
      "Loss: Var(v=48.8227, grad=0.0000)\n"
     ]
    }
   ],
   "source": [
    "# Sequence loss functions\n",
    "\n",
    "def squared_loss_sequence(t, y):\n",
    "    \n",
    "    # t: target sequences\n",
    "    # y: predicted sequences\n",
    "\n",
    "    # Both should be of shape [batch_size][sequence_length][features]\n",
    "    \n",
    "    # add check that sizes agree\n",
    "    assert len(t) == len(y), \"Target and prediction batch sizes must match\"\n",
    "    \n",
    "    def squared_loss_single(t_seq, y_seq):\n",
    "        \n",
    "        # t_seq: single target sequence\n",
    "        # y_seq: single predicted sequence\n",
    "\n",
    "        # add check that sizes agree\n",
    "        assert len(t_seq) == len(y_seq), \"Target and prediction sequence lengths must match\"\n",
    "        \n",
    "        Loss = Var(0.0)\n",
    "        \n",
    "        # Loop over all time steps in the sequence\n",
    "        for i in range(len(t_seq)):\n",
    "            assert len(t_seq[i]) == len(y_seq[i]), \"Feature dimensions must match\"\n",
    "            # Loop over all features in the current time step\n",
    "            for j in range(len(t_seq[i])):\n",
    "                diff = t_seq[i][j] - y_seq[i][j]\n",
    "                Loss += diff * diff\n",
    "        \n",
    "        return Loss\n",
    "\n",
    "    Loss = Var(0.0)\n",
    "    \n",
    "    # Loop over all sequences in the batch\n",
    "    for i in range(len(t)):\n",
    "        Loss += squared_loss_single(t[i], y[i])\n",
    "\n",
    "    return Loss\n",
    "\n",
    "def cross_entropy_loss_sequence(t, y):\n",
    "    # t: target sequences (one-hot)\n",
    "    # y: predicted sequences \n",
    "    #     = logits\n",
    "    #     = outputs of last layer\n",
    "    #     -> we will apply softmax inside the loss function\n",
    "\n",
    "    # Check that the batch sizes agree\n",
    "    assert len(t) == len(y), \"Target and prediction batch sizes must match\"\n",
    "\n",
    "    def cross_entropy_loss_single(t_seq, y_seq):\n",
    "        \n",
    "        # softmax_i = exp(logit_i) / sum_j exp(logits_j)\n",
    "        # cross_entropy = - sum_i t_i * log(softmax_i)\n",
    "        \n",
    "        # t_seq: single target sequence\n",
    "        # y_seq: single predicted sequence\n",
    "\n",
    "        # add check that sizes agree\n",
    "        assert len(t_seq) == len(y_seq), \"Target and prediction sequence lengths must match\"\n",
    "\n",
    "        Loss = Var(0.0)\n",
    "        \n",
    "        # Loop over all time steps in the sequence\n",
    "        for i in range(len(t_seq)):\n",
    "            assert len(t_seq[i]) == len(y_seq[i]), \"Feature dimensions must match\"\n",
    "            \n",
    "            # Compute softmax denominator for current time step\n",
    "            denominator = Var(0.0)\n",
    "\n",
    "            for j in range(len(y_seq[i])):\n",
    "                denominator += y_seq[i][j].exp()\n",
    "            \n",
    "            # Compute cross-entropy loss for current time step\n",
    "            # Loop over all features in the current time step\n",
    "            for j in range(len(t_seq[i])):\n",
    "                # Only compute loss for true class (one-hot encoding)\n",
    "                if t_seq[i][j].v > 0:  \n",
    "                    Loss += -(y_seq[i][j] - denominator.log())\n",
    "        return Loss\n",
    "\n",
    "    Loss = Var(0.0)\n",
    "    \n",
    "    for i in range(len(t)):\n",
    "        Loss += cross_entropy_loss_single(t[i], y[i])\n",
    "\n",
    "    return Loss\n",
    "\n",
    "def sequence_loss(t: Sequence[Sequence[Var]], y: Sequence[Sequence[Var]], loss_fn=cross_entropy_loss_sequence) -> Var:\n",
    "    assert len(t) == len(y)\n",
    "    return loss_fn(t, y)\n",
    "\n",
    "\n",
    "# Test of loss func\n",
    "NN = [\n",
    "    RNNLayer(4, 2, lambda x: x.tanh()),\n",
    "    DenseLayer(2, 4, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "output_train = forward_batch(encoded_training_set_x[:3], NN)\n",
    "print(output_train)       \n",
    "loss = sequence_loss(output_train, encoded_training_set_y[:3], cross_entropy_loss_sequence)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezSRiVJzk2h5"
   },
   "source": [
    "# Backpropagation through time \n",
    "\n",
    "Since we have **automatic differentiation** we don't have to code the backpropagation rule by hand. Just to give you a bit of appreciation for have much bookkeeping is necessary we have given the derivation belwo.\n",
    "\n",
    "We need to compute the partial derivatives\n",
    "$\n",
    "\\frac{\\partial E}{\\partial W},~\\frac{\\partial E}{\\partial U},~\\frac{\\partial E}{\\partial V}\n",
    "$. \n",
    "We repeat the definition of the RNN forward pass from above:\n",
    "\n",
    "- $h_t = f(U\\,{x_t} + V\\,{h_{t-1}})$, where $f$ usually is an activation function, e.g. $\\mathrm{tanh}$.\n",
    "- $o_t = W\\,{h_t}$\n",
    "- $\\hat{y}_t = \\mathrm{softmax}(o_{t})$\n",
    "\n",
    "where\n",
    "- $U$ is a weight matrix applied to the given input sample,\n",
    "- $V$ is a weight matrix used for the recurrent computation in order to pass memory along the sequence,\n",
    "- $W$ is a weight matrix used to compute the output of the every timestep (given that every timestep requires an output), and\n",
    "- $h$ is the hidden state (the network's memory) for a given time step.\n",
    "\n",
    "---\n",
    "\n",
    "Recall though, that RNNs are **recurrent** and the **weights** **$W,~U,~V$ are shared across time**, i.e. **we do not have separate weights for each time step**. \n",
    "\n",
    "---\n",
    "\n",
    "We define a loss function\n",
    "\n",
    "- $E = \\sum_t E_t  = \\sum_t E_t(y_t ,\\hat{y}_t ) \\ , $\n",
    "\n",
    "where $E_t(y_t ,\\hat{y}_t )$ is the cross-entropy function:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "E_t(y_t ,\\hat{y}_t ) &= - \\sum_{i=1}^{c} y_{t,i} \\log(\\hat{y}_{t,i}) \\\\\n",
    "&= - \\sum_{i=1}^{c} y_{t,i} \\log\\left(\\frac{\\exp(o_{t,i})}{\\sum_{j=1}^{c} \\exp(o_{t,j})}\\right) \\\\\n",
    "&= - \\sum_{i=1}^{c} y_{t,i} \\left(o_{t,i} - \\log \\left(\\sum_{j=1}^{c} \\exp(o_{t,j})\\right)\\right) \\\\\n",
    "&= - \\sum_{i=1}^{c} y_{t,i} o_{t,i} + \\log\\left(\\sum_{j=1}^{c} \\exp(o_{t,j})\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Therefore, to compute e.g. the partial derivative $\\frac{\\partial E}{\\partial W}$, we need to 1) sum up across time, and 2) apply the chain rule:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial W} = \\sum_{t} \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_{t}}{\\partial W}\\,.$$\n",
    "\n",
    "To compute $\\frac{\\partial o_{t}}{\\partial W}$ we use the definition of $o_t$ above.\n",
    "From week 1 (exercise i) we have that\n",
    "$$\\boxed{\\delta_{o,t} \\equiv \\frac{\\partial E}{\\partial o_{t}}} = \\frac{\\partial E_t}{\\partial o_{t}} = \\hat{y}_{t} - y_{t}\\,,$$\n",
    "where $\\hat{y}_{t}$ is a softmax distribution over model outputs $o_{t}$ at time $t$, and $y_{t}$ is the target label at time $t$. \n",
    "\n",
    "Why is $\\frac{\\partial E_t}{\\partial o_{t}} = \\hat{y}_{t} - y_{t}$?\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial E_t}{\\partial o_{t,i}} &= - \\sum_{k=1}^{c} y_{t,k} \\frac{\\partial o_{t,k}}{\\partial o_{t,i}} + \\frac{1}{\\sum_{j=1}^{c} \\exp(o_{t,j})} \\sum_{j=1}^{c} \\exp(o_{t,j}) \\frac{\\partial o_{t,j}}{\\partial o_{t,i}} \\\\\n",
    "&= - y_{t,i} + \\frac{\\exp(o_{t,i})}{\\sum_{j=1}^{c} \\exp(o_{t,j})} \\\\\n",
    "&= \\hat{y}_{t,i} - y_{t,i} \\ .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "To compute $\\frac{\\partial E}{\\partial U}$ and $\\frac{\\partial E}{\\partial V}$ we again sum over time and use the chain rule:\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial U} = \\sum_{t} \\frac{\\partial E}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial U} \\ . \n",
    "$$\n",
    "This leads us to introduce\n",
    "$$\n",
    "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} \\ .\n",
    "$$\n",
    "The backpropagation through time recursion is derived by realising that a variation of $h_t$ affects 1) the loss at time step $t$ through the feed-forward connection to the output and 2) the future losses through the $h_{t+1}$ dependence of $h_t$. Mathematically, we write this through the chain rule:\n",
    "\n",
    "$$\n",
    "\\delta_{h,t} \\equiv \\frac{\\partial E}{\\partial h_{t}} =  \\frac{\\partial E}{\\partial o_{t}} \\frac{\\partial o_t}{\\partial h_{t}} + \\frac{\\partial E}{\\partial h_{t+1}}\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} = \\delta_{o,t} \\frac{\\partial o_t}{\\partial h_{t}} + \\delta_{h,t+1}\n",
    "\\frac{\\partial h_{t+1}}{\\partial h_{t}} \\ . \n",
    "$$\n",
    "\n",
    "Like above we can compute $\\frac{\\partial h_{t+1}}{\\partial h_{t}}$ using the definition of the network (shifted one time step). In the code the intermediate steps to compute the $\\delta$ recursions have been precomputed for you. \n",
    "\n",
    "For more information on backpropagation through time see the [Goodfellow book section 10.2.2](https://www.deeplearningbook.org/contents/rnn.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIy3OZaQSrVL"
   },
   "source": [
    "# Exercise f) Complete the training loop\n",
    "\n",
    "Complete the training loop above and run the training. You can leave the hyper-parameters and network size unchanged.\n",
    "\n",
    "Note that despite the small size of the network and dataset, training still takes quite a while. This is an issue with the recurrent structure of Nanograd. Using PyTorch, we would be able to use much larger datasets and models. We will attempt that in the bottom of the notebook. For now, you should get a feel of the recurrent structure of the RNN under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "MkaqbWmroncY"
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = 200\n",
    "LR = 1e-2 \n",
    "LR_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training\n",
    "def parameters(network):\n",
    "    \"\"\"Extract all parameters from a network\"\"\"\n",
    "    params = []\n",
    "    for layer in network:\n",
    "        params.extend(layer.parameters())\n",
    "    return params\n",
    "\n",
    "def update_parameters(params, learning_rate=0.01):\n",
    "    \"\"\"Update parameters using gradient descent\"\"\"\n",
    "    for param in params:\n",
    "        param.v -= learning_rate * param.grad\n",
    "\n",
    "def zero_gradients(params):\n",
    "    \"\"\"Zero out all gradients\"\"\"\n",
    "    for param in params:\n",
    "        param.grad = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "-JtM_IQjonfK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 ( 0.00%) Train loss: 1.376 \t Validation loss: 1.495\n",
      "   5 ( 2.50%) Train loss: 1.366 \t Validation loss: 1.490\n",
      "  10 ( 5.00%) Train loss: 1.357 \t Validation loss: 1.484\n",
      "  15 ( 7.50%) Train loss: 1.349 \t Validation loss: 1.479\n",
      "  20 (10.00%) Train loss: 1.341 \t Validation loss: 1.474\n",
      "  25 (12.50%) Train loss: 1.334 \t Validation loss: 1.470\n",
      "  30 (15.00%) Train loss: 1.328 \t Validation loss: 1.465\n",
      "  35 (17.50%) Train loss: 1.322 \t Validation loss: 1.461\n",
      "  40 (20.00%) Train loss: 1.317 \t Validation loss: 1.457\n",
      "  45 (22.50%) Train loss: 1.312 \t Validation loss: 1.453\n",
      "  50 (25.00%) Train loss: 1.308 \t Validation loss: 1.449\n",
      "  55 (27.50%) Train loss: 1.303 \t Validation loss: 1.446\n",
      "  60 (30.00%) Train loss: 1.299 \t Validation loss: 1.442\n",
      "  65 (32.50%) Train loss: 1.295 \t Validation loss: 1.439\n",
      "  70 (35.00%) Train loss: 1.292 \t Validation loss: 1.436\n",
      "  75 (37.50%) Train loss: 1.289 \t Validation loss: 1.433\n",
      "  80 (40.00%) Train loss: 1.285 \t Validation loss: 1.430\n",
      "  85 (42.50%) Train loss: 1.282 \t Validation loss: 1.427\n",
      "  90 (45.00%) Train loss: 1.280 \t Validation loss: 1.424\n",
      "  95 (47.50%) Train loss: 1.277 \t Validation loss: 1.422\n",
      " 100 (50.00%) Train loss: 1.274 \t Validation loss: 1.419\n",
      " 105 (52.50%) Train loss: 1.272 \t Validation loss: 1.417\n",
      " 110 (55.00%) Train loss: 1.270 \t Validation loss: 1.415\n",
      " 115 (57.50%) Train loss: 1.268 \t Validation loss: 1.412\n",
      " 120 (60.00%) Train loss: 1.266 \t Validation loss: 1.410\n",
      " 125 (62.50%) Train loss: 1.264 \t Validation loss: 1.408\n",
      " 130 (65.00%) Train loss: 1.262 \t Validation loss: 1.406\n",
      " 135 (67.50%) Train loss: 1.260 \t Validation loss: 1.404\n",
      " 140 (70.00%) Train loss: 1.258 \t Validation loss: 1.402\n",
      " 145 (72.50%) Train loss: 1.257 \t Validation loss: 1.400\n",
      " 150 (75.00%) Train loss: 1.255 \t Validation loss: 1.399\n",
      " 155 (77.50%) Train loss: 1.254 \t Validation loss: 1.397\n",
      " 160 (80.00%) Train loss: 1.252 \t Validation loss: 1.395\n",
      " 165 (82.50%) Train loss: 1.251 \t Validation loss: 1.394\n",
      " 170 (85.00%) Train loss: 1.250 \t Validation loss: 1.392\n",
      " 175 (87.50%) Train loss: 1.248 \t Validation loss: 1.391\n",
      " 180 (90.00%) Train loss: 1.247 \t Validation loss: 1.389\n",
      " 185 (92.50%) Train loss: 1.246 \t Validation loss: 1.388\n",
      " 190 (95.00%) Train loss: 1.245 \t Validation loss: 1.387\n",
      " 195 (97.50%) Train loss: 1.244 \t Validation loss: 1.385\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(int(np.ceil(len(encoded_training_set_x)/batch_size))):\n",
    "        # Forward pass and loss computation\n",
    "\n",
    "        start = b*batch_size\n",
    "        end = min((b+1)*batch_size, len(encoded_training_set_x))\n",
    "        # Here we use mini-batches of size `batch_size`\n",
    "        # Also, we use the stored hidden state\n",
    "        output = forward_batch(encoded_training_set_x[start:end], NN, use_stored_hid=False)\n",
    "        Loss = sequence_loss(encoded_training_set_y[start:end], output, cross_entropy_loss_sequence)\n",
    "        \n",
    "        # Backward pass\n",
    "        Loss.backward()\n",
    "        \n",
    "        # gradient descent update\n",
    "        update_parameters(parameters(NN), LR)\n",
    "        zero_gradients(parameters(NN))\n",
    "      \n",
    "    LR = LR * LR_DECAY\n",
    "\n",
    "    # Training loss\n",
    "    # Here, we use the entire training set to compute the training loss\n",
    "    # (not mini-batches as above)\n",
    "    # Also, we don't use the stored hidden state\n",
    "    # (we want to compute the loss over the entire sequences)\n",
    "    output = forward_batch(encoded_training_set_x, NN, use_stored_hid=False)\n",
    "    Loss = sequence_loss(encoded_training_set_y, output, cross_entropy_loss_sequence)\n",
    "    train_loss.append(Loss.v/len(encoded_training_set_y))\n",
    "        \n",
    "    # Validation loss\n",
    "    Loss_validation = Var(0.0)\n",
    "    output = forward_batch(encoded_validation_set_x, NN, use_stored_hid=False)\n",
    "    Loss_validation = sequence_loss(encoded_validation_set_y, output, cross_entropy_loss_sequence)\n",
    "    val_loss.append(Loss_validation.v/len(encoded_validation_set_y))\n",
    "    \n",
    "    if e%5==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n",
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "nAI_D6g25pTQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "['a', 'a', 'a', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['a', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/qw26xkk51853rtytqd9ns_5r0000gn/T/ipykernel_48735/161328427.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y = [[Var(float(x[i,j])) for j in range(x.shape[1])] for i in range(x.shape[0])]\n"
     ]
    }
   ],
   "source": [
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size)\n",
    "\n",
    "# Forward pass\n",
    "outputs = forward_batch(encoded_test_set_x[:1], NN)\n",
    "\n",
    "output_sentence = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]\n",
    "\n",
    "print('Input sentence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nn7QpUZXk2iH"
   },
   "source": [
    "## Exercise g) Extrapolation\n",
    "\n",
    "Now that we have trained an RNN, it's time to put it to test. We will provide the network with a starting sentence and let it `freestyle` from there!\n",
    "\n",
    "How well does your RNN extrapolate -- does it work as expected? Are there any imperfections? If yes, why could that be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GNsD6HEJ-Gn"
   },
   "outputs": [],
   "source": [
    "def freestyle(NN, sentence='', num_generate=10):\n",
    "    \"\"\"\n",
    "    Takes in a sentence as a string and outputs a sequence\n",
    "    based on the predictions of the RNN.\n",
    "    \n",
    "    Args:\n",
    "     `params`: the parameters of the network\n",
    "     `sentence`: string with whitespace-separated tokens\n",
    "     `num_generate`: the number of tokens to generate\n",
    "    \"\"\"\n",
    "    sentence = sentence.split(' ')\n",
    "    output_sentence = sentence\n",
    "    sentence_one_hot = one_hot_encode_sequence(sentence, vocab_size)\n",
    "\n",
    "    # Begin predicting\n",
    "    outputs = forward_batch([sentence_one_hot], NN, use_stored_hid=False)\n",
    "    output_words = [idx_to_word[np.argmax(output)] for output in Var_to_nparray(outputs[0])]\n",
    "    word = output_words[-1]\n",
    "\n",
    "    # Append first prediction\n",
    "    output_sentence.append(word)\n",
    "\n",
    "    # Forward pass - Insert code here!\n",
    "    if word != 'EOS':\n",
    "      for i in range(num_generate-1):\n",
    "          sentence_one_hot = \n",
    "          outputs = \n",
    "          output_words = \n",
    "          word = \n",
    "          output_sentence.append(word)\n",
    "          if word == 'EOS':\n",
    "              break\n",
    "          \n",
    "    return output_sentence\n",
    "\n",
    "\n",
    "# Perform freestyle (extrapolation)\n",
    "test_examples = ['a a b', 'a a a a b', 'a a a a a a b', 'a', 'r n n']\n",
    "for i, test_example in enumerate(test_examples):\n",
    "    print(f'Example {i}:', test_example)\n",
    "    print('Predicted sequence:', freestyle(NN, sentence=test_example), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X44hQ653vNCj"
   },
   "source": [
    "# Introduction to the Long Short-Term Memory (LSTM) Cell\n",
    "\n",
    "Reading material: [Christopher Olah's walk-through](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "A vanilla RNN suffers from [the vanishing gradients problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem) which gives challenges in saving memory over longer sequences. To combat these issues the gated hidden units were created. The two most prominent gated hidden units are the Long Short-Term Memory (LSTM) cell and the Gated Recurrent Unit (GRU), both of which have shown increased performance in saving and reusing memory in later timesteps. In this exercise, we will focus on LSTM but you would easily be able to go ahead and implement the GRU as well based on the principles that you learn here.\n",
    "\n",
    "Below is a figure of the LSTM cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Rgc-g3zwV9f"
   },
   "source": [
    "![lstm](https://i.imgur.com/3VkmUCe.png)\n",
    "Source: https://arxiv.org/abs/1412.7828"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytasZ5cqw4W1"
   },
   "source": [
    "\n",
    "The LSTM cell contains three gates, input, forget, output gates and a memory cell.\n",
    "The output of the LSTM unit is computed with the following functions, where $\\sigma = \\mathrm{sigmoid}$.\n",
    "We have input gate $i$, forget gate $f$, and output gate $o$ defines as\n",
    "\n",
    "- $i = \\sigma ( W^i [h_{t-1}, x_t])$\n",
    "\n",
    "- $f = \\sigma ( W^f [h_{t-1},x_t])$\n",
    "\n",
    "- $o = \\sigma ( W^o [h_{t-1},x_t])$\n",
    "\n",
    "where $W^i, W^f, W^o$ are weight matrices applied to a concatenated $h_{t-1}$ (hidden state vector) and $x_t$ (input vector)  for each respective gate.\n",
    "\n",
    "$h_{t-1}$, from the previous time step along with the current input $x_t$ are used to compute the a candidate $g$\n",
    "\n",
    "- $g = \\mathrm{tanh}( W^g [h_{t-1}, x_t])$\n",
    "\n",
    "The value of the cell's memory, $c_t$, is updated as\n",
    "\n",
    "- $c_t = c_{t-1} \\circ f + g \\circ i$\n",
    "\n",
    "where $c_{t-1}$ is the previous memory, and $\\circ$ refers to element-wise multiplication (hint: element-wise multiplication is computed with the `*` operator in numpy).\n",
    "\n",
    "The output, $h_t$, is computed as\n",
    "\n",
    "- $h_t = \\mathrm{tanh}(c_t) \\circ o$\n",
    "\n",
    "and it is used for both the timestep's output and the next timestep, whereas $c_t$ is exclusively sent to the next timestep.\n",
    "This makes $c_t$ a memory feature, and is not used directly to compute the output of the timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8_4RWp3k2iQ"
   },
   "source": [
    "## Exercise h) Make the LSTMLayer class\n",
    "\n",
    "Make the LSTM class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdU0yMXQU7d0"
   },
   "outputs": [],
   "source": [
    "# Insert code here\n",
    "\n",
    "class LSTMLayer:\n",
    "    def __init__(self, n_in: int, n_hid: int, act_fn, initializer = NormalInitializer(), initializer_hid = NormalInitializer()):\n",
    "        self.n_in = n_in\n",
    "        self.n_hid = n_hid\n",
    "        self.g_layer = \n",
    "        self.i_layer = \n",
    "        self.f_layer = \n",
    "        self.o_layer = \n",
    "        self.initial_hid = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.stored_hid = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.initial_c = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.stored_c = [Var(0.0) for _ in range(n_hid)]\n",
    "        self.act_fn = act_fn\n",
    "    \n",
    "    def __repr__(self):    \n",
    "        return 'Feed-forward: ' + repr(self.in_hid_layer) + ' Candidate: ' + repr(self.g_layer) + ' i gate ' + repr(self.i_layer) + ' f gate ' + repr(self.f_layer) + ' o gate ' + repr(self.o_layer) + ' Initial hidden: ' + repr(self.initial_hid)\n",
    "\n",
    "    def parameters(self) -> Sequence[Var]:      \n",
    "      return self.in_hid_layer.parameters() + self.g_layer.parameters() + self.i_layer.parameters() + self.f_layer.parameters() + self.o_layer.parameters() + self.initial_hid\n",
    "\n",
    "    def forward_step(self, input: Sequence[Var], input_hid: Sequence[Var], input_c: Sequence[Var]) -> Sequence[Var]:\n",
    "        hids = []\n",
    "        cs = []\n",
    "        concatenated_input = []\n",
    "        for val in input_hid:\n",
    "          concatenated_input.append(val)\n",
    "        for val in input:\n",
    "          concatenated_input.append(val)\n",
    "\n",
    "        # Insert code here\n",
    "\n",
    "        return hids, cs\n",
    "    \n",
    "    def forward_sequence(self, input: Sequence[Sequence[Var]], use_stored_hid = False) -> Sequence[Sequence[Var]]:\n",
    "        out = []\n",
    "        if use_stored_hid:\n",
    "            hid = self.stored_hid\n",
    "            c = self.stored_c\n",
    "        else:\n",
    "            hid = self.initial_hid\n",
    "            c = self.initial_c\n",
    "        # Takes a sequence and loops over each character in the sequence. Note that each character has dimenson equal to the embeddng dimenson\n",
    "        for i in range(len(input)):\n",
    "            hid, c = # insert code here\n",
    "            out.append(hid)\n",
    "        self.stored_hid = hid\n",
    "        self.stored_c = c\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKu-bfhzk2iY"
   },
   "source": [
    "Here is a bit of code to test it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4AYroqSVRSv"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    LSTMLayer(1, 5, lambda x: x.tanh()),\n",
    "    DenseLayer(5, 1, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "print(NN[0])\n",
    "x_train =[[[Var(1.0)], [Var(2.0)], [Var(3.0)]],\n",
    "          [[Var(1.0)], [Var(2.0)], [Var(3.0)]]]\n",
    "\n",
    "output_train = forward_batch(x_train, NN)          \n",
    "output_train[0][0][0].backward()\n",
    "\n",
    "print(output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4r4mgFsk2ik"
   },
   "source": [
    "## Exercise i) LSTM training\n",
    "\n",
    "Complete the LSTM training loop\n",
    "\n",
    "Run the training loop. Training time in Nanograd will likely be long, but see if you can find settings to compare your LSTM learning curve (NLL and number of epochs) to the vanilla RNN from earlier. Do you observe any improvements? Motivate your answer.\n",
    "\n",
    "Finally, below we will implement LSTM in PyTorch. You will notice it is much, much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MOAmppJD66tJ"
   },
   "outputs": [],
   "source": [
    "# Initialize training hyperparameters\n",
    "EPOCHS = \n",
    "LR = \n",
    "LR_DECAY = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiotu2ab66w-"
   },
   "outputs": [],
   "source": [
    "NN = [\n",
    "    LSTMLayer(4, 1, lambda x: x.tanh()),\n",
    "    DenseLayer(1, 4, lambda x: x.identity())\n",
    "]\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    for b in range(int(np.ceil(len(encoded_training_set_x)/batch_size))):\n",
    "        # Forward pass and loss computation\n",
    "        Loss =\n",
    "        # Backward pass\n",
    "        Loss.backward()\n",
    "        \n",
    "        # gradient descent update\n",
    "        update_parameters(parameters(NN), LR)\n",
    "        zero_gradients(parameters(NN))\n",
    "      \n",
    "    LR = LR * LR_DECAY\n",
    "\n",
    "    # Training loss\n",
    "    Loss = \n",
    "    train_loss.append(Loss.v/len(encoded_training_set_y))\n",
    "        \n",
    "    # Validation loss\n",
    "    Loss_validation = \n",
    "    val_loss.append(Loss_validation.v/len(encoded_validation_set_y))\n",
    "    \n",
    "    if e%5==0:\n",
    "        print(\"{:4d}\".format(e),\n",
    "              \"({:5.2f}%)\".format(e/EPOCHS*100), \n",
    "              \"Train loss: {:4.3f} \\t Validation loss: {:4.3f}\".format(train_loss[-1], val_loss[-1]))\n",
    "        \n",
    "# Plot training and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "epoch = np.arange(len(train_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, val_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gi51eWgKxyOk"
   },
   "source": [
    "## PyTorch implementation of the LSTM\n",
    "\n",
    "Now that we know how the LSTM cell works, let's see how easy it is to use in PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6HDdJLuk2ip"
   },
   "source": [
    "Definition of our LSTM network. We define a LSTM layer using the [nn.LSTM](https://pytorch.org/docs/stable/nn.html#lstm) class. The LSTM layer takes as argument the size of the input and the size of the hidden state like in our Nanograd implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UGrvknfk2ip"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyRecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "        \n",
    "        # Recurrent layer\n",
    "        # YOUR CODE HERE!\n",
    "        self.lstm = \n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=50,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # RNN returns output and last hidden state\n",
    "        x, (h, c) = self.lstm(x)\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6r3bPwYk2is"
   },
   "source": [
    "## Exercise j) Train in PyTorch\n",
    "\n",
    "Define an LSTM for our recurrent neural network `MyRecurrentNet` above. A single LSTM layer is sufficient. What should the input size and hidden size be? Hint: use the PyTorch documentation.\n",
    "\n",
    "It's time for us to train our network. In the section below, you will get to put your deep learning skills to use and create your own training loop. You may want to consult previous exercises if you cannot recall how to define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets redefine the one_hot_encode() and one_hot_encode_sequence() functions\n",
    "# to work now with torch.Tensor variables instead of Var.\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.array([np.zeros(vocab_size)])\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[0][idx] = 1.0\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D torch.Tensor of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[2], 1)\n",
    "    return torch.Tensor(encoding)\n",
    "\n",
    "# lets test it\n",
    "test_word = one_hot_encode(word_to_idx['a'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a\\' has shape {test_word.shape}.')\n",
    "\n",
    "test_sentence = one_hot_encode_sequence(['a', 'b'], vocab_size)\n",
    "print(f'Our one-hot encoding of \\'a b\\' has shape {test_sentence.shape}.')\n",
    "\n",
    "print(test_word)\n",
    "print(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2URKsyFDx8xG"
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "\n",
    "# Initialize a new network\n",
    "net = MyRecurrentNet()\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "# YOUR CODE HERE!\n",
    "criterion = \n",
    "optimizer = \n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "    net.eval()\n",
    "        \n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE!\n",
    "        outputs = \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = \n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().numpy()\n",
    "    \n",
    "    net.train()\n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "        targets_idx = [word_to_idx[word] for word in targets]\n",
    "        \n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "        \n",
    "        # Convert target to tensor\n",
    "        targets_idx = torch.LongTensor(targets_idx)\n",
    "        \n",
    "        # Forward pass\n",
    "        # YOUR CODE HERE!\n",
    "        outputs = \n",
    "        \n",
    "        # Compute loss\n",
    "        # YOUR CODE HERE!\n",
    "        loss = \n",
    "        \n",
    "        # Backward pass\n",
    "        # YOUR CODE HERE!\n",
    "        # zero grad, backward, step...\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 10 epochs\n",
    "    if i % 10 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "        \n",
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size)\n",
    "targets_idx = [word_to_idx[word] for word in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)\n",
    "\n",
    "# Forward pass\n",
    "outputs = net.forward(inputs_one_hot).data.numpy()\n",
    "\n",
    "print('\\nInput sequence:')\n",
    "print(inputs)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(targets)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_word[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydr7Czg_k2iw"
   },
   "source": [
    "# Exercise k) Compare PyTorch and Nanograd implementations\n",
    "\n",
    "Compare the two implementations (in terms of predictive performance, training speed, etc.). Are they similar? How do they differ?\n",
    "\n",
    "\n",
    "Try to play around with the choice of hyper-parameters, optimizer, and hidden dimensions. How much can you improve the negative log-likelihood by these simple changes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M93ORx95k2ix"
   },
   "source": [
    "## Exercise l) Other RNN cells (optional)\n",
    "\n",
    "Aside from the LSTM cell, various other RNN cells exist. The gated recurrent unit (GRU) is a variation of the LSTM cell that uses less gating mechanisms. Try to look it up in the [PyTorch documentation](https://pytorch.org/docs/stable/nn.html#gru) and switch out the LSTM cell in the code above. What do you notice in terms of performance and convergence speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juN400Ekk2iz"
   },
   "source": [
    "## Exercise m) More complex tasks (optional)\n",
    "\n",
    "Go back and generate a more complex patterned dataset to learn from. Do you see any significant differences between a vanilla RNN and LSTM (implemented in e.g. PyTorch) when you increase the difficulty of the task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v68YEkEBk2iz"
   },
   "source": [
    "# It works, now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjpqSrSuk2i0"
   },
   "source": [
    "In this notebook you have learned how to use embeddings, recurrent neural networks, and the LSTM cell in particular.\n",
    "\n",
    "As we have already seen, RNNs are excellent for sequential data such as language. But what do we do if we're modelling data with strong dependency in both directions? Like in many things deep learning, we can build powerful models by stacking layers on top of each other; *bi-directional* RNNs consist of two LSTM cells, one for each direction. A sequence is first fed into the forward LSTM cell and the reversed sequence is then used as input to the backward LSTM cell together with the last hidden state from the forward LSTM cell. Follow [this link](https://pdfs.semanticscholar.org/4b80/89bc9b49f84de43acc2eb8900035f7d492b2.pdf) for the original paper from 1997(!).\n",
    "\n",
    "For even deeper representations, multiple layers of both uni-directional and bi-directional RNNs can be stacked ontop of each other, just like feed-forward and convolutional layers. For more information on this, check out the [LSTM PyTorch documentation](https://pytorch.org/docs/stable/nn.html#lstm). Next week we will also explore ways to combine RNNs with other types of layers for even more expressive function approximators."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bdA4LPsFiACe",
    "cGSoDRgHk2g1",
    "Dzmryk72k2g-",
    "M93ORx95k2ix"
   ],
   "name": "5_2_Recurrent_Neural_Networks_Nanograd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DTU_deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
