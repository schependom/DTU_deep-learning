{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb542cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bec70",
   "metadata": {},
   "source": [
    "# Autograd\n",
    "\n",
    "Computational graph:\n",
    "\n",
    "- **Input** tensors are the **leaves**\n",
    "- **Output** tensor is the **root**\n",
    "- Every **operation** is a **node**\n",
    "\n",
    "When you call `.backward()` on the output tensor, autograd traverses the graph from the root to the leaves, calculating and storing the gradients of each **leaf** (input) tensor in its `.grad` attribute.\n",
    "\n",
    "If you call `.retain_grad()` on a non-leaf tensor (an intermediate result), its gradient will also be computed and stored in its `.grad` attribute during the backward pass. If not, only the gradients of leaf tensors will be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32269fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True)     # leaf node (input)\n",
    "y = x + 2                                   # non-leaf (intermediate) result\n",
    "z = y * y * 3                               # result = root node\n",
    "out = z.mean()\n",
    "\n",
    "y.retain_grad() # retain gradients for non-leaf (intermediate) nodes\n",
    "out.backward()  # compute gradients, starting from root z and going backwards to leaves\n",
    "\n",
    "# print(z.grad) # None, because z doesn't have a next node\n",
    "print(y.grad)   # d(out)/d(y) with y \\in R^{2x2} (only because we called y.retain_grad())\n",
    "print(x.grad)   # d(out)/d(x) with x \\in R^{2x2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca9c2a",
   "metadata": {},
   "source": [
    "`x.grad` computes the gradient of `out` with respect to $\\mathbf{x} = \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 \\end{bmatrix}$:\n",
    "$$\n",
    "\\nabla_{x} \\texttt{out} = \\begin{bmatrix} \\dfrac{\\partial \\texttt{out}}{\\partial \\mathbf{x}_1} & \\dfrac{\\partial \\texttt{out}}{\\partial \\mathbf{x}_2} \\end{bmatrix} = \\begin{bmatrix}\n",
    "\\dfrac{\\partial \\texttt{out}}{\\partial x_{11}} & \\dfrac{\\partial \\texttt{out}}{\\partial x_{12}} \\\\\n",
    "\\dfrac{\\partial \\texttt{out}}{\\partial x_{21}} & \\dfrac{\\partial \\texttt{out}}{\\partial x_{22}} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405db534",
   "metadata": {},
   "source": [
    "Let's denote the tensor `out` with $o$:\n",
    "\n",
    "$$o = \\frac{1}{4} \\sum_{i,j} z_{ij}, \\qquad z_{ij} = 3y^2, \\qquad y = x_{ij} + 2$$\n",
    "\n",
    "Let's compute the derivative, given $x_{ij} = 1$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o}{\\partial x_{ij}} & = \\frac{\\partial o}{\\partial z_{ij}} \\frac{\\partial z_{ij}}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial x_{ij}} \\\\\n",
    "& = \\frac{1}{4} \\cdot 6 (x_{ij}+2) \\cdot 1 \\\\\n",
    "& = \\frac{3}{2} (x_{ij}+2) \\\\\n",
    "& = \\frac{3}{2} (1+2) \\\\\n",
    "& = \\frac{9}{2} = 4.5\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's treat the tensor as a flattened vector $\\mathbf{x} \\in \\mathbb{R}^4$.\n",
    "We define the chain of functions:\n",
    "1. $\\mathbf{y} = \\mathbf{x} + 2\\mathbf{1}$\n",
    "2. $\\mathbf{z} = 3(\\mathbf{y} \\odot \\mathbf{y})$\n",
    "3. $o = \\frac{1}{4} \\sum z_i = \\frac{1}{4} \\mathbf{1}^\\top \\mathbf{z}$\n",
    "\n",
    "Using the multivariate Chain Rule, the gradient is the product of Jacobians:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial o}{\\partial \\mathbf{x}} = \\frac{\\partial o}{\\partial \\mathbf{z}} \\cdot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} \\cdot \\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}}\n",
    "$$\n",
    "\n",
    "Let's calculate the three Jacobians step-by-step.\n",
    "\n",
    "**Step 1 (Mean):** The derivative of a sum/mean is a row vector.\n",
    "$$\n",
    "\\frac{\\partial o}{\\partial \\mathbf{z}} = \\begin{bmatrix} \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2 (Element-wise Square):** Since $z_i$ only depends on $y_i$, the off-diagonal entries are 0. This results in a **Diagonal Jacobian Matrix**.\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}} = \\begin{bmatrix}\n",
    "\\frac{\\partial z_1}{\\partial y_1} & 0 & \\dots & 0 \\\\\n",
    "0 & \\frac{\\partial z_2}{\\partial y_2} & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\dots & \\frac{\\partial z_4}{\\partial y_4}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "6y_1 & 0 & 0 & 0 \\\\\n",
    "0 & 6y_2 & 0 & 0 \\\\\n",
    "0 & 0 & 6y_3 & 0 \\\\\n",
    "0 & 0 & 0 & 6y_4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Step 3 (Linear Add):** The derivative is the Identity matrix.\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\mathbf{I}_4\n",
    "$$\n",
    "\n",
    "Multiplying the row vector by the diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o}{\\partial \\mathbf{x}} &= \\begin{bmatrix} \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} & \\frac{1}{4} \\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "6y_1 & 0 & 0 & 0 \\\\\n",
    "0 & 6y_2 & 0 & 0 \\\\\n",
    "0 & 0 & 6y_3 & 0 \\\\\n",
    "0 & 0 & 0 & 6y_4\n",
    "\\end{bmatrix} \\mathbf{I}_4 \\\\\n",
    "&= \\begin{bmatrix} \\frac{1}{4}(6y_1) & \\frac{1}{4}(6y_2) & \\frac{1}{4}(6y_3) & \\frac{1}{4}(6y_4) \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Given input $x_{ij} = 1 \\implies y_{ij} = 3$:\n",
    "$$\n",
    "= \\begin{bmatrix} 4.5 & 4.5 & 4.5 & 4.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "In Autograd engines (like PyTorch), we never explicitly construct the Jacobian matrix $\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}$ for element-wise operations because it would be an $N \\times N$ matrix with mostly zeros.\n",
    "\n",
    "Mathematically, multiplying a vector $\\mathbf{v}$ by a diagonal matrix $\\text{diag}(\\mathbf{u})$ is equivalent to the **Hadamard (element-wise) product**:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}^\\top \\cdot \\text{diag}(\\mathbf{u}) \\equiv (\\mathbf{v} \\odot \\mathbf{u})^\\top\n",
    "$$\n",
    "\n",
    "This is why we can simplify the notation to $\\frac{\\partial o}{\\partial \\mathbf{z}} \\odot \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{y}}$, as done in simpler derivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afb65305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3858, 0.2446, 0.4414],\n",
      "        [0.7755, 0.0104, 0.1654]], requires_grad=True)\n",
      "tensor([[0.7716, 0.4891, 0.8828],\n",
      "        [1.5511, 0.0208, 0.3308]], grad_fn=<MulBackward0>)\n",
      "Gradient of my_sum w.r.t. my_tensor: tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "Gradient of my_sum w.r.t. x: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Gradient of my_sum w.r.t. itself: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dn/qw26xkk51853rtytqd9ns_5r0000gn/T/ipykernel_15226/3036280057.py:20: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"Gradient of my_sum w.r.t. itself: {my_sum.grad}\") # result = root node, so it does not have any gradients\n"
     ]
    }
   ],
   "source": [
    "my_tensor = torch.rand(2,3, requires_grad=True)\n",
    "\n",
    "print(my_tensor)\n",
    "\n",
    "# multiply by two and assign the result to a new variable\n",
    "x = my_tensor.multiply(2)\n",
    "\n",
    "print(x)\n",
    "\n",
    "# sum the variables elements\n",
    "my_sum = x.sum()\n",
    "\n",
    "# for intermediate steps, we can call .retain_grad() to keep track of the gradients\n",
    "x.retain_grad()\n",
    "\n",
    "# perform a backward pass on the last variable\n",
    "my_sum.backward()\n",
    "print(f\"Gradient of my_sum w.r.t. my_tensor: {my_tensor.grad}\") # input always has gradients\n",
    "print(f\"Gradient of my_sum w.r.t. x: {x.grad}\") # x is not a leaf node but we called x.retain_grad()\n",
    "print(f\"Gradient of my_sum w.r.t. itself: {my_sum.grad}\") # result = root node, so it does not have any gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6bbb3c",
   "metadata": {},
   "source": [
    "> !\n",
    "> The shape of the gradient `.grad` attribute **matches the shape of the original tensor.**\n",
    "> !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a0a91",
   "metadata": {},
   "source": [
    "# Defining Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f8351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # ------------------\n",
    "        # define layers here\n",
    "        # ------------------\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        # ------------------------\n",
    "        # define forward pass here\n",
    "        # ------------------------\n",
    "\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        return x \n",
    "        # or return softmax(x, dim=1) for classification\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b897c8",
   "metadata": {},
   "source": [
    "Parameters and gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9ef419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print parameters\n",
    "list(net.named_parameters())\n",
    "list(net.parameters())\n",
    "\n",
    "# Access a specific parameter\n",
    "param = net.Wi\n",
    "print(param.shape)\n",
    "print(param.requires_grad)\n",
    "print(param.is_leaf) # not created by an operation (e.g. W)\n",
    "print(param.data)\n",
    "print(param.grad) \n",
    "\n",
    "\n",
    "# Calling .forward()\n",
    "output = net(input)\n",
    "\n",
    "# Compute gradients\n",
    "output.backward(torch.randn(num_input, num_output))\n",
    "# backward() only requires an argument when the output is not a scalar\n",
    "\n",
    "# Set gradients to zero\n",
    "# This is important because by default, gradients are accumulated in PyTorch.\n",
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b8a3e",
   "metadata": {},
   "source": [
    "Loss and optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d2880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute loss\n",
    "loss = nn.MSELoss()(output, target)\n",
    "loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "# Define optimizer:\n",
    "optimizer = optim.SGD(\n",
    "net.parameters(),   # what parameters to optimize\n",
    "lr=0.01,            # learning rate \\alpha\n",
    "momentum=0.9,       # momentum \\beta\n",
    "weight_decay=1e-4,  # L2 regularization\n",
    ")\n",
    "# 1. SGD + Momentum:    learning rate 0.01 - 0.1 \n",
    "# 2. ADAM:              learning rate 3e-4 - 1e-5\n",
    "# 3. RMSPROP:           somewhere between SGD and ADAM\n",
    "\n",
    "# Zero gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Update parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcc0679",
   "metadata": {},
   "source": [
    "\n",
    "### Ballpark estimates of hyperparameters\n",
    "- __Number of hidden units and network structure:__\n",
    "\n",
    "    One rarely goes below **512** units for feedforward networks (unless your are training on CPU...). In general trial and error.\n",
    "\n",
    "- __Parameter initialization:__\n",
    "    Parameter initialization is extremely important.\n",
    "    Often used initializer are   \n",
    "    1. Kaiming He\n",
    "    2. Xavier Glorot\n",
    "    3. Uniform or Normal with small scale (0.1 - 0.01)\n",
    "    4. Orthogonal (this usually works very well for RNNs)\n",
    "\n",
    "    Bias is nearly always initialized to zero using `torch.nn.init.constant(tensor, val)`\n",
    "\n",
    "- __Mini-batch size:__\n",
    "\n",
    "    Usually people use 16-256. Bigger is not allways better. With smaller mini-batch size you get more updates and your model might converge faster. Also small batch sizes use less memory, which means you can train a model with more parameters.\n",
    "\n",
    "- __Nonlinearity:__ The most commonly used nonliearities are\n",
    "    1. ReLU\n",
    "    2. Leaky ReLU\n",
    "    3. ELU\n",
    "    4. Sigmoid (rarely, if ever, used in hidden layers anymore, squashes the output to the interval [0, 1] - appropriate if the targets are binary.\n",
    "    5. Tanh is similar to the sigmoid, but squashes to [-1, 1]. Rarely used any more.\n",
    "    6. Softmax normalizes the output to 1, usrful if you have a multi-class classification problem.\n",
    "\n",
    "- __Optimizer and learning rate:__\n",
    "    1. SGD + Momentum: learning rate 0.01 - 0.1 \n",
    "    2. ADAM: learning rate 3e-4 - 1e-5\n",
    "    3. RMSPROP: somewhere between SGD and ADAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0277a",
   "metadata": {},
   "source": [
    "Defining a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd4b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, num_features, num_hidden, num_output):\n",
    "    super(Net, self).__init__()\n",
    "\n",
    "    # Define activation function\n",
    "    self.activation = F.relu # nn.ReLU()\n",
    "    # x = F.leaky_relu(z, negative_slope=0.01)\n",
    "    # x = F.elu(x)\n",
    "    # x = torch.sigmoid(x)\n",
    "    # x = torch.tanh(x)\n",
    "    \n",
    "    # Using nn.Linear...\n",
    "\n",
    "    self.fc1 = nn.Linear(num_features, num_hidden)\n",
    "    self.fc2 = nn.Linear(num_hidden, num_output)\n",
    "\n",
    "    # Or manually...\n",
    "\n",
    "    # OPT1: Xavier-Glorot initialization: \\sigma = sqrt(2 / (in + out))\n",
    "    #   ! Remember: size = (out, in) = (num_rows, num_columns) in x @ W.T + b\n",
    "    #   ! Here we go from num_features --> num_hidden\n",
    "    self.W1 = nn.Parameter(init.xavier_normal_(torch.Tensor(num_hidden, num_features)))\n",
    "    self.b1 = nn.Parameter(init.constant_(torch.Tensor(num_hidden), 0))\n",
    "\n",
    "    # OPT2: Kaiming-He initialization: \\sigma = sqrt(2 / in)\n",
    "    #   ! Remember: size = (out, in) = (num_rows, num_columns) in x @ W.T + b\n",
    "    #   ! Here we go from num_hidden --> num_output\n",
    "    self.W2 = nn.Parameter(init.kaiming_normal_(torch.Tensor(num_output, num_hidden)))\n",
    "    self.b2 = nn.Parameter(init.constant_(torch.Tensor(num_output), 0))\n",
    "\n",
    "    # Dropout\n",
    "    self.dropout1 = nn.Dropout(p=0.5) # no size dependence\n",
    "    self.dropout2 = nn.Dropout(p=0.5) # no size dependence\n",
    "\n",
    "    # BatchNorm\n",
    "    self.batchnorm1 = nn.BatchNorm1d(num_hidden) # size of layer\n",
    "    self.batchnorm2 = nn.BatchNorm1d(num_output) #   to which we apply batchnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2889535a",
   "metadata": {},
   "source": [
    "Using the layers in `forward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5eceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self, x):\n",
    "\n",
    "    # ---- HIDDEN LAYER 1 ----\n",
    "\n",
    "    # Apply fully connected layer...\n",
    "    x = self.fc1(x)                      # ...using nn.Linear\n",
    "    x = F.linear(x, self.W1, self.b1)    # ... or manually x @ W.T + b\n",
    "\n",
    "    # Apply batchnorm BEFORE activation function\n",
    "    x = self.batchnorm1(x)\n",
    "\n",
    "    # Apply nonlinearity\n",
    "    x = self.activation(x)\n",
    "\n",
    "    # Apply dropout AFTER activation function\n",
    "    x = self.dropout1(x)\n",
    "\n",
    "    # ---- HIDDEN LAYER 2 ----\n",
    "\n",
    "    x = self.fc2(x)                      # ...using nn.Linear\n",
    "    x = F.linear(x, self.W2, self.b2)    # ... or manually x @ W.T + b\n",
    "\n",
    "    # Apply batchnorm BEFORE activation function\n",
    "    x = self.batchnorm2(x)\n",
    "\n",
    "    # Apply activation function\n",
    "    x = self.activation(x)\n",
    "\n",
    "    # Apply dropout AFTER activation function\n",
    "    x = self.dropout2(x)\n",
    "\n",
    "    # ----- OUTPUT LAYER -----\n",
    "\n",
    "    x = self.fc3(x)                      # ...using nn.Linear\n",
    "    x = F.linear(x, self.W3, self.b3)    # ... or manually x @ W.T + b\n",
    "\n",
    "    # NO SOFTMAX HERE \n",
    "    #   IF USING CrossEntropyLoss\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f26bf",
   "metadata": {},
   "source": [
    "Using the layers in `forward()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(num_features, num_hidden, num_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfcb26a",
   "metadata": {},
   "source": [
    "L1 regularization requires manually adding the sum of absolute values of parameters to the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d1738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization\n",
    "l1_lambda = 1e-5\n",
    "l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "\n",
    "# MANUALLY add l1_lambda * l1_norm to your loss function during training.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target) + l1_lambda * l1_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b5d38",
   "metadata": {},
   "source": [
    "CrossEntropyLoss expects raw **logits**, not probabilities.\n",
    "It applies SoftMax **internally** before computing the cross-entropy loss. That's why we didn't apply softmax in the last layer of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b67a18",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99409d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done this ourselves,\n",
    "# but we should be aware of sklearn and its tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# setting hyperparameters and gettings epoch sizes\n",
    "batch_size = 100\n",
    "num_epochs = 200\n",
    "num_samples_train = x_train.shape[0]\n",
    "num_batches_train = num_samples_train // batch_size\n",
    "num_samples_valid = x_valid.shape[0]\n",
    "num_batches_valid = num_samples_valid // batch_size\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_acc, train_loss = [], []\n",
    "valid_acc, valid_loss = [], []\n",
    "test_acc, test_loss = [], []\n",
    "cur_loss = 0\n",
    "losses = []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "\n",
    "    ########\n",
    "    ## Train\n",
    "    ########\n",
    "\n",
    "    cur_loss = 0 # sum all batch losses\n",
    "\n",
    "    # set network to training mode\n",
    "    net.train()\n",
    "\n",
    "    # loop over all mini-batches\n",
    "    for i in range(num_batches_train):\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        slce = get_slice(i, batch_size)\n",
    "        output = net(x_train[slce])\n",
    "        \n",
    "        # compute gradients given loss\n",
    "        target_batch = targets_train[slce]\n",
    "\n",
    "        batch_loss = criterion(output, target_batch) # criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        #-----------\n",
    "        ## If using L1 regularization, add the L1 penalty to the loss\n",
    "        # l1_lambda = 1e-5\n",
    "        # l1_norm = sum(p.abs().sum() for p in net.parameters())\n",
    "        # batch_loss += l1_lambda * l1_norm\n",
    "        #-----------\n",
    "\n",
    "        batch_loss.backward() # backpropagate gradients\n",
    "        optimizer.step() # update parameters after this batch of data\n",
    "        \n",
    "        cur_loss += batch_loss\n",
    "\n",
    "    # append average loss per batch for the epoch\n",
    "    losses.append(cur_loss / batch_size)\n",
    "\n",
    "    # set network to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    #####################\n",
    "    ### Evaluate training\n",
    "    #####################\n",
    "\n",
    "    train_preds, train_targs = [], []\n",
    "\n",
    "    # loop over all mini-batches \n",
    "    for i in range(num_batches_train):\n",
    "\n",
    "        slce = get_slice(i, batch_size)\n",
    "\n",
    "        # get outputs of final network (after training with all mini-batches, but within the epoch)\n",
    "        output = net(x_train[slce])\n",
    "        preds = torch.max(output, 1)[1] # most likely class indices\n",
    "        train_targs += list(targets_train[slce].numpy())\n",
    "        train_preds += list(preds.data.numpy())\n",
    "    \n",
    "    #######################\n",
    "    ### Evaluate validation\n",
    "    #######################\n",
    "\n",
    "    val_preds, val_targs = [], []\n",
    "\n",
    "    for i in range(num_batches_valid):\n",
    "\n",
    "        slce = get_slice(i, batch_size)\n",
    "\n",
    "        # get outputs of final network (after training with all mini-batches, but within the epoch)\n",
    "        output = net(x_valid[slce])\n",
    "        preds = torch.max(output, 1)[1] # most likely class indices\n",
    "        val_targs += list(targets_valid[slce].numpy())\n",
    "        val_preds += list(preds.data.numpy())\n",
    "        \n",
    "\n",
    "    # compute accuracy\n",
    "    train_acc_cur = accuracy_score(train_targs, train_preds)\n",
    "    valid_acc_cur = accuracy_score(val_targs, val_preds)\n",
    "    \n",
    "    train_acc.append(train_acc_cur)\n",
    "    valid_acc.append(valid_acc_cur)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch %2i : Train Loss %f , Train acc %f, Valid acc %f\" % (\n",
    "                epoch+1, losses[-1], train_acc_cur, valid_acc_cur))\n",
    "\n",
    "epoch = np.arange(len(train_acc))\n",
    "plt.figure()\n",
    "plt.plot(epoch, train_acc, 'r', epoch, valid_acc, 'b')\n",
    "plt.legend(['Train Accucary','Validation Accuracy'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b07a2",
   "metadata": {},
   "source": [
    "Convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da425254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "    \n",
    "    super(Model, self).__init__()\n",
    "\n",
    "    # ---\n",
    "    # CONVOLUTIONAL layer\n",
    "    #   to change number of channels\n",
    "    # ---\n",
    "\n",
    "    # Keep spatial dimensions (uneven k!)\n",
    "    stride = 1\n",
    "    padding = (kernel_size - 1) // 2\n",
    "\n",
    "    # Convert   x \\in R^{batch_size x in_channels x H x W}\n",
    "    # to        x \\in R^{batch_size x out_channels x H' x W'}\n",
    "    self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "    \n",
    "    # ---\n",
    "    # POOLING layer\n",
    "    #   to reduce spatial dimensions H and W,\n",
    "    #   introduce translational invariance,\n",
    "    #   reduce computation by factor 4\n",
    "    # ---\n",
    "\n",
    "    # Convert   x \\in R^{batch_size x out_channels x H' x W'}\n",
    "    # to        x \\in R^{batch_size x out_channels x H'/2 x W'/2}\n",
    "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "    # Halve H and W\n",
    "\n",
    "    # ---\n",
    "    # FULLY CONNECTED layer\n",
    "    #   for the output (classification/regression)\n",
    "    # ---\n",
    "\n",
    "    # Convert  x \\in R^{batch_size x (out_channels * H'' * W'')}\n",
    "    # to       x \\in R^{batch_size x num_output}\n",
    "    flattened_size = out_channels * H_final * W_final\n",
    "    self.fc = nn.Linear(flattened_size, num_output)\n",
    "\n",
    "    # Or a deeper (classification) network with more FC layers\n",
    "    self.fc = nn.Sequential(\n",
    "        nn.Flatten(),                      \n",
    "        nn.Linear(flattened_size, hidden_size),    \n",
    "        activation_fn,\n",
    "        nn.Linear(hidden_size, self.num_classes),   \n",
    "        # No softmax here, as CrossEntropyLoss does that internally\n",
    "    )\n",
    "\n",
    "def forward(self, x):\n",
    "\n",
    "    # x \\in R^{batch_size x in_channels x H x W}\n",
    "    identity = x\n",
    "\n",
    "    # Apply convolutional layer\n",
    "    out = self.conv(x)          # R^{batch_size x out_channels x H' x W'}\n",
    "    out = self.bn(out)          # apply batchnorm (optional)\n",
    "\n",
    "    # Change number of channels in identity\n",
    "    if identity.shape[1] != out.shape[1]:\n",
    "        identity = self.conv_identity(identity) # 1x1 conv to change channels\n",
    "\n",
    "    # residual connection\n",
    "    out = out + identity        \n",
    "\n",
    "    # Apply activation function\n",
    "    out = F.relu(out)\n",
    "\n",
    "    # Apply pooling layer\n",
    "    out = self.pool(out)          # R^{batch_size x out_channels x H'/2 x W'/2}\n",
    "\n",
    "    # Apply fully connected layer \n",
    "    # (flattening is done inside this layer)\n",
    "    out = self.fc(out)            # R^{batch_size x num_output}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ce6bf",
   "metadata": {},
   "source": [
    "In order to keep $H$ and $W$ the same after a convolution with kernel size $k$ and stride $s=1$, use padding $p=\\frac{k-1}{2}$ for odd $k$:\n",
    "```python\n",
    "# Assuming s=1, k=odd\n",
    "conv_padding = (kernel_size - 1) // 2  # for odd kernel_size\n",
    "```\n",
    "\n",
    "For even $k$, it is not possible to keep the same size with symmetric padding.\n",
    "\n",
    "In case of stride $s>1$, the output size is given by:\n",
    "$$H_{out} = \\left\\lfloor \\frac{H_{in} + 2p - k}{s} \\right\\rfloor + 1, \\quad W_{out} = \\left\\lfloor \\frac{W_{in} + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "In this case of $s>1$, we cannot keep the output size the same as the input size. Stride implies downsampling, i.e. skipping pixels. If $s=2$, we keep roughly half the pixels in each dimension: $$H_{out} \\approx \\frac{H_{in}}{s}, \\quad W_{out} \\approx \\frac{W_{in}}{s}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa6599",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "He intialization normally sets $$\\text{Var}(w) = \\frac{2}{\\text{num\\_in}}$$\n",
    "However, in RNN's, since we have both input and hidden contributions, and they have unit variance, their sum has variance 2. Therefore, to keep the overall variance of the pre-activation at 1, we set: $$\\text{Var}(w) = \\frac{1}{\\text{num\\_in}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN in PyTorch\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])  # get the last time step\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed21ea",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "In PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de4dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRecurrentNet(nn.Module):\n",
    "\n",
    "    # A simple LSTM for predicting the next word in a sequence\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(MyRecurrentNet, self).__init__()\n",
    "\n",
    "        # vocab_size = number of unique words in vocabulary\n",
    "        \n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size,\n",
    "                            hidden_size=50,         # number of features in hidden state\n",
    "                            num_layers=1,\n",
    "                            batch_first=True)       # input & output tensors are provided as \n",
    "                                                    # (batch, seq, feature)\n",
    "        \n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=50,      # must match hidden_size above\n",
    "                            out_features=vocab_size,# must match number of classes\n",
    "                            bias=False)             # no bias needed here as we have a bias in the LSTM layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # LSTM.forward() returns output x and LAST hidden state (h, c)\n",
    "\n",
    "        x, (h, c) = self.lstm(x)\n",
    "\n",
    "        # x = output features from the last layer of the LSTM, for each timestep\n",
    "        #       -> per batch, this is a sequence of (h_0, h_1, ..., h_{seq_length-1})\n",
    "        #       -> shape (batch_size, seq_length, hidden_size)\n",
    "        #\n",
    "        # h = hidden state for last timestep t=seq_length\n",
    "        #\n",
    "        # c = cell state for last timestep t=seq_length\n",
    "        \n",
    "        # Flatten output for feed-forward layer\n",
    "        x = x.view(-1, self.lstm.hidden_size)\n",
    "\n",
    "        # view takes 2 arguments: \n",
    "        #   -> num_rows, num_columns\n",
    "        #   -> -1 means \"infer this dimension based on the other one\"\n",
    "        \n",
    "        # Now, x has shape (batch_size*seq_length, hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.l_out(x)\n",
    "\n",
    "        # l_out returns a 2D tensor of shape (batch_size*seq_length, vocab_size)\n",
    "        # and contains the logits for each class (word in vocabulary)\n",
    "        \n",
    "        return x\n",
    "\n",
    "net = MyRecurrentNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff44b93",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af83c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -math.inf)\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"A simple Multi-head attention layer.\"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n",
    "        self.attn = None # store the attention maps\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        nbatches = query.size(0)\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e065382d",
   "metadata": {},
   "source": [
    "### Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fefce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))` (not \\sigma * I!)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "\n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=128, out_features=2*latent_features) \n",
    "            #! note the 2*latent_features !!\n",
    "            # we predict \\mu \\in R^d and \\log(\\sigma) \\in R^d (we don't assume \\sigma * I)\n",
    "        )\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=self.observation_features)\n",
    "        )\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        # we learn log(sigma) because sigma must be positive\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits = self.decoder(z)\n",
    "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
    "        return Bernoulli(logits=px_logits, validate_args=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "    \n",
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "    \n",
    "vi = VariationalInference(beta=1.0)\n",
    "loss, diagnostics, outputs = vi(vae, images)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTU_deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
